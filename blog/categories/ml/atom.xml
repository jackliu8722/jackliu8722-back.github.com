<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[博客分类: 机器学习 | 精神兵的 Blog]]></title>
  <link href="http://jackliu8722.github.com/blog/categories/ml/atom.xml" rel="self"/>
  <link href="http://jackliu8722.github.com/"/>
  <updated>2013-10-21T21:25:18+08:00</updated>
  <id>http://jackliu8722.github.com/</id>
  <author>
    <name><![CDATA[jackliu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[主成份分析(PCA)]]></title>
    <link href="http://jackliu8722.github.com/blog/2013/10/16/pca/"/>
    <updated>2013-10-16T08:19:00+08:00</updated>
    <id>http://jackliu8722.github.com/blog/2013/10/16/pca</id>
    <content type="html"><![CDATA[<p>主成份分析（Principal components analysis，PCA）是一种分析、简化数据集的技术。PCA在应用中使用得非常广泛，比如：降维、有损数据压缩、特征提取以及数据可视化。</p>

<p>对PCA的定义有两种常用的方法，但是最终都产生了相同的算法。第一种定义为将一个高维的数据次投影到低维的线性空间上，这样的低维空间被称为主体子空间，投影后，使数据的方差最大化，该定义可称为最大方差理论。等价地，</p>

<h3>最大方差理论</h3>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[EM算法]]></title>
    <link href="http://jackliu8722.github.com/blog/2013/09/08/emsuan-fa/"/>
    <updated>2013-09-08T20:30:00+08:00</updated>
    <id>http://jackliu8722.github.com/blog/2013/09/08/emsuan-fa</id>
    <content type="html"><![CDATA[<h2>1 简介</h2>

<p>一直对EM算法有一种莫明的喜欢感，觉得它非常的神奇，其思想也非常的让人深思，给我的感觉就是一种美。</p>

<p>EM算法是最大期望算法(Exception-maximization algorithm)的简称。它是一种迭代算法，用于求含有隐变量的概率模型参数的极大似然估计。</p>

<p>EM算法经过两个步骤交替进行计算，从而求得概率模型的极大似然估计：</p>

<ol>
<li><p>第一步计算期望，也就是E，利用对隐变量的现在估计值，计算模型的极大似然估计值。</p></li>
<li><p>第二步是最大化过程，也就是M。该过程在第一步基础上计算模型到达极大值时参数的值。</p></li>
</ol>


<h2>2 EM算法的推导</h2>

<h4>2.1 算法的引入</h4>

<p>假设我们记n个观测样本为
\[x_1,x_2,x_3,…,x_k,…,x_n\]
<strong>注意:</strong>一般我们假设各样本之间满足独立同分布(independent and identically distributed, i.i.d)。</p>

<p>观测样本即为观测变量，可将所有的观测变量记为<strong>X</strong>。相应的，观测变量对应的隐变量为
\[z_1,z_2,z_3,…,z_k,…z_n\]
同样的，假设隐变量之间满足独立同分布，记所有的隐变量为<strong>Z</strong>。观测变量与隐变量的联合分布为 <strong>$$ P(X,Z |\theta )$$</strong></p>

<p>其中， $$$\theta$$$是模型的参数。</p>

<p>一般地，我们可以将观测变量X称为不完全数据，将(X,Z)称为完全数据(complete-data)。我们的目标是最大化$$$P(X|\theta)$$$，而
\[P(X|\theta)= \sum_{Z}P(X,Z|\theta) \]</p>

<p>假设Z是离散的（如果Z是连续的，可以用积分代替求和）。</p>

<p>在最大化$$$P(X|\theta)$$$的时候，如果不存在隐变量，我们可以直接通过最大似然估计求解模型的参数$$$\theta$$$，由于隐变量Z的存在，求解模型参数的最优值就变得比较困难了。但是求解完全数据的最大似然函数$$$P(X,Z|\theta)$$$的最优解是比较容易的。</p>

<h4>2.2 算法的推导</h4>

<p>从2.1节我们已经知道，我们的目标是最大化$$$P(X|\theta)$$$，而我们可以做如下的一些推导。
\[ P(X|\theta)={P(X,Z|\theta) \over P(Z|X,\theta) }\]
两边取对数
\[ \ln P(X|\theta)=\ln {P(X,Z|\theta) \over P(Z|X,\theta)}\]
即可得到
\[ \ln P(X|\theta)=\ln P(X,Z|\theta)  - \ln P(Z|X,\theta)\]
假设有分布$$$\ln q(Z)$$$，则有
\[ \ln P(X|\theta)=\ln P(X,Z|\theta)  - \ln q(Z) + \ln q(Z) - \ln P(Z|X,\theta)\]</p>

<p>\[ \ln P(X|\theta)=(\ln P(X,Z|\theta) - \ln q(Z)) - (\ln P(Z|X,\theta)-\ln q(Z))\]</p>

<p>\[ \ln P(X|\theta)={\ln {P(X,Z|\theta) \over q(Z)}} - {\ln {P(Z|X,\theta) \over q(Z)}}\]
等式两边同乘以$$$q(Z)$$$，并对Z求积分(Z是连续随机变量)或者求和(Z是离散随机变量)，假设Z是连接的，则有
\[ \int_{Z} q(Z)\ln P(X|\theta) \,dZ = \int_{Z}q(Z)({\ln {P(X,Z|\theta) \over q(Z)}} - {\ln {P(Z|X,\theta) \over q(Z)}})\,dZ \]</p>

<p>\[ \int_{Z} q(Z)\ln P(X|\theta) \,dZ = {\int_{Z}q(Z){\ln {P(X,Z|\theta) \over q(Z)}}\,dZ} - {\int_{Z}q(Z)\ln {P(Z|X,\theta) \over q(Z)}}\,dZ \]</p>

<p>\[ \int_{Z} q(Z)\ln P(X|\theta) \,dZ = {\int_{Z}q(Z){\ln {P(X,Z|\theta) \over q(Z)}}\,dZ} + {\int_{Z}q(Z)\ln {q(Z) \over P(Z|X,\theta)}}\,dZ \]</p>

<p>由于Z和X之间相互独立，因此有
\[ \int_{Z}q(Z)\ln P(X|\theta)\,dZ = \int_{Z}q(Z)\,dZ \ln(X|\theta) \]
而$$$\int_{Z}q(Z)\,dZ = 1$$$，因此有 $$$\int_{Z}q(Z)q(Z)\ln P(X|\theta)\,dZ = ln(X|\theta)$$$，则有
\[ \ln P(X|\theta) = {\int_{Z}q(Z){\ln {P(X,Z|\theta) \over q(Z)}}\,dZ} + {\int_{Z}q(Z)\ln {q(Z) \over P(Z|X,\theta)}}\,dZ \]
记
\[ \zeta(q,\theta)={\int_{Z}q(Z){\ln {P(X,Z|\theta) \over q(Z)}}\,dZ}\]</p>

<p>\[ KL(q||p) = {\int_{Z}q(Z)\ln {q(Z) \over P(Z|X,\theta)}}\,dZ\]
KL是KL距离，也就是Kullback-Leibler差异（Kullback-Leibler Divergence）的简称，也叫做相对熵（Relative Entropy）。它衡量的是相同事件空间里的两个概率分布的差异情况。其物理意义是：在相同事件空间里，概率分布P(x)的事件空间，若用概率分布Q(x)编码时，平均每个基本事件（符号）编码长度增加了多少比特。KL的一个重要性质是 $$$KL(P|Q)\ge0$$$，当且仅当$$$P=Q$$$时，等号成立。</p>

<p>则有\[ \ln P(X|\theta) = \zeta(q,\theta) + KL(q||p) \]</p>

<p>由于\[ KL(q||p) \ge 0 \]
则有\[ \ln P(X|\theta) \ge \zeta(q,\theta) \]
也就是说$$$\zeta(q,\theta)$$$是$$$\ln P(X|\theta)$$$的下界。</p>

<p>EM算法可以通过不断提高下界$$$\zeta(q,\theta)$$$的值来逼近$$$\ln P(X|\theta)$$$的最大值，因此EM算法分为以下两步通过迭代的方法求$$$\ln P(X|\theta)$$$的最大似然估计值：</p>

<ol>
<li>在E步中，假设当前的参数为$$$\theta ^{old}$$$，并固定参数$$$\theta ^{old}$$$不变，则最大化下界$$$\zeta(q,\theta ^{old})$$$与$$$q(Z)$$$有关，并且$$$\ln P(X|\theta ^{old})$$$与$$$q(Z)$$$不相关，因此，要使$$$\zeta(q,\theta)$$$最大，等价于使$$$KL(q||p)$$$最小，由KL距离的性质可知 $$$q(Z)=P(Z|X,\theta ^{old})$$$</li>
<li>在M步中，固定$$$q(Z)$$$，最大化下界$$$\zeta(q,\theta)$$$与$$$\theta$$$有关，即寻找一个新的值$$$\theta ^{new}$$$使得$$$\zeta(q,\theta ^{new})$$$最大。$$$\theta ^{new}$$$会使下界$$$\zeta(q,\theta)$$$的值增大（除非已经达到最大值），并且也一定会使似然函数的值增大，因为在M步中，分布q(Z)使用的是参数$$$\theta ^{old}$$$而不是参数$$$\theta ^{new}$$$，则$$$q(Z)\ne P(Z|X,\theta ^{new}) $$$，从而$$$KL(q||p)> 0$$$，因此，似然函数增大部分的值比仅提高下界多带来的增幅大，从而能够逼近$$$\ln P(X|\theta)$$$的最大似然估计。</li>
</ol>


<p>在最大化$$$\zeta(q,\theta)$$$的过程中，由 $$$q(Z)=P(Z|X,\theta ^{old})$$$ 可知
\[ \zeta(q,\theta)={\int_{Z}P(Z|X,\theta ^{old}){\ln {P(X,Z|\theta) \over P(Z|X,\theta ^{old})}}\,dZ}\]
\[ \zeta(q,\theta)={\int_{Z}P(Z|X,\theta ^{old})\ln P(X,Z|\theta)}\,dZ - {\int_{Z}P(Z|X,\theta ^{old})\ln P(Z|X,\theta ^{old})}\,dZ\]
由于$$${\int_{Z}P(Z|X,\theta ^{old})\ln P(Z|X,\theta ^{old})}\,dZ$$$为常数，则有
\[ \zeta(q,\theta)={\int_{Z}P(Z|X,\theta ^{old})\ln P(X,Z|\theta)}\,dZ + const \]
因此最大化$$$\zeta(q,\theta)$$$等价于最大化$$${\int_{Z}P(Z|X,\theta ^{old})\ln P(X,Z|\theta)}\,dZ$$$</p>

<h2>3 说明</h2>

<p>关于EM算法，作以下几点说明：</p>

<ol>
<li>参数的初始值可以任意选择，但需要注意的是EM算法对参数的初值是敏感的。</li>
<li>EM算法中，E步是就函数$$${\int_{Z}P(Z|X,\theta ^{old})\ln P(X,Z|\theta)}\,dZ$$$，我们可以定义Q函数 \[ Q(\theta,\theta ^{k}) = {\int_{Z}P(Z|X,\theta ^{k})\ln P(X,Z|\theta)}\,dZ\] 表示完全数据的对数似然函数$$$\ln P(Y,Z|\theta)$$$关于给定观测数据$$$Y$$$和给定参数$$$$\theta ^{k}$$$的条件下对未观测数据$$$Z$$$的条件概率分布$$$P(Z|X,\theta ^{k})$$$的期望，因此，Q函数也可以写成如下形式\[ Q(\theta,\theta ^{k}) = E_{Z}[\ln P(X,Z|\theta)|X,\theta ^{k}]\] 变元$$$\theta$$$表示需要极大化的参数，变元$$$\theta ^{k}$$$表示参数当前的估计值。</li>
<li>EM算法中，M步是在求$$$Q(\theta,\theta ^{k})$$$的极大化，完成一次迭代$$$\theta ^{k} \rightarrow\theta ^{k+1}$$$。</li>
<li>EM算法迭代的停止条件一般为，给定较小的正数$$$\varepsilon _{1},\varepsilon _{2}$$$，若满足 \[\parallel \theta ^{k+1} - \theta ^{k} \parallel &lt; \varepsilon _{1}\] 或 \[\parallel Q(\theta ^{k+1},\theta ^{k}) - Q(\theta ^{k},\theta ^{k}) \parallel &lt; \varepsilon_{2}\] 则停止迭代。每次迭代其实是在求Q函数及其极大化。</li>
</ol>

]]></content>
  </entry>
  
</feed>
